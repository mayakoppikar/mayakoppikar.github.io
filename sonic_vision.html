<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Sonic Vision - Maya Koppikar</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" />
</head>
<body class="bg-light">

<div class="container py-5">
  <!-- Top Section: All text on left, images on right -->
  <div class="row mb-5">
    <!-- Text Column -->
    <div class="col-md-8">
      <h2>Sonic Vision: A Head-mounted Depth Perception Device</h2>
      <p>This project is a tool for visually impaired users to percieve depths in short ranges using haptic feedback from servo motors. I worked on this project as a team of 3 engineers as part of the Cockrell School Design Makeathon. This project was took us around 2 days to complete (October 2023). This project was selected as a finallist by the Makeathon panel. </p>
<br>
      <h6>Key Features</h6>
      <ul>
        <li>Low-cost head-mounted device that provides haptic feedback to blind users</li>
        <li>Control algorithm + time delays to synchronize array of ultrasonic sensors</li>
        <li>Cross-sensor minimization subroutines</li>
      </ul>

      <h6>Tech Stack</h6>
      <ul>
        <li>Programming: C# </li>
        <li>Hardware: Arduino Uno Microcontroller</li>
        <li>Environment: Arduino IDE </li>
      </ul>

     <br><br>

<!--<h6>Design Process</h6>
<ol>
  <li>
    <strong>Memory Management:</strong>
    We implemented a File Allocation Table (FAT) file system abstraction to manage memory. In order to implement processes in our RTOS, we worked on a SPI driver for an SD card. Doing so, I learned about the SPI protocol. Later we wrote code to implement the FAT, which involves creating a directory to store file metadata (file names, sizes, starting locations) and 512-byte chunks where data is stored.

 
    <img src="images/spi_packets.png" alt="SPI Packet" class="img-fluid my-3" style="max-width:300px;">
  </li>
<br>
  <li>
    <strong>Sensor Data Acquisition:</strong>
    We continually read in 3 distance sensor values per sample: one in the center pointing straight ahead, and two 45 degrees on the left/right. This gives us 3 data points to generate coefficients for a corresponding parabola. We calculate the vertex of the parabola and use inverse trig to find the angle to that vertex. The magnitude of this angle is used in the motor control algorithm. We mainly use the TF Luna LIDAR sensor, as it provides smooth, accurate digital readings via UART.


    <img src="images/sensors.png" alt="Sensors" class="img-fluid my-3" style="max-width:300px;">
    <img src="images/tfluna.png" alt="TF Luna LIDAR" class="img-fluid my-3" style="max-width:300px;">
  </li>
<br>
  <li>
    <strong>Motor Control and Race Strategy:</strong>
    We began with a call-graph to formulate a high-level strategy. Motors are controlled using differential steering. We apply a base power to both motors, and then use the input angle from LIDAR sensors to calculate an offset value. When turning right, we add the offset to the left wheel and subtract from the right (and vice versa). The offset is refined using a PD control algorithm before being applied as the PWM duty cycle.

  
    <img src="images/call_graph.png" alt="Call Graph" class="img-fluid my-3" style="max-width:300px;">
    <img src="images/algo.png" alt="Algorithm" class="img-fluid my-3" style="max-width:300px;">
  </li>
<br>
  <li>
    <strong>Testing & Tuning:</strong>
    Once the kernel, sensors, and motors were fully initialized, we tuned the PD control. Our team set up multiple race tracks, measured the robot's response to sharp turns, and adjusted the Kp and Kd values of our control algorithm accordingly.
  </li>
  <br>
</ol>
-->


      <h6>Major Challenges & Key Takeaways</h6>
      <ul>
        <li>Cross-Sensor Interference: We collected data from 3 ultrasonic sensors in an array. The close proximity of these sensors resulted in noisy data which we resolved by adding time delays and storing intermediate results in a buffer. We implemented algorithms to smooth the data using median filtering and omitting outliers.</li>
        <br>
        <li>Key Takeaways + Skils:
          <ul>
            <li>Interfacing sensors</li>
            <li>Smoothing data using software algorithms</li>
          </ul>
        </li>
      </ul>
    </div>
<br>
    <!-- Images Column -->
    <div class="col-md-4">
      <img src="images/sonic_setup.JPG" alt="Sonic Set-up" class="img-fluid mb-3">
      <img src="images/sonic_vision.JPG" alt="Sonic Vision" class="img-fluid mb-3">
    </div>
  </div>

  <!-- Demos Section -->
  <section id="currentprojects" class="container py-5">
    <h2>Demos</h2>
    <div class="row">

      <!-- Demo 1 -->
      <div class="col-md-4 mb-4">
        <h4>Demo</h4>
        <div class="ratio ratio-16x9 mb-3">
          <iframe src="https://drive.google.com/file/d/1Gqv2hmJU_UJxDXpPUeS1Oh76Z5sveptt/preview" 
                  title="Sonic Vision Demo" 
                  allowfullscreen></iframe>
        </div>
        <p>November 12,2025: A demo of Sonic Vision. As an object moves clsoer to the device, the servo motors oscilaltes with an increased frequency which we programmed using a mapping control function. </p>
      </div>

    </div>
  </section>
<a href="index.html" class="btn btn-secondary mb-5">‚Üê Back to Portfolio</a>
</div>



<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
